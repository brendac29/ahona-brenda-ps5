---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}

```

  
### 2. Crawling (PARTNER 1)

```{python}

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime
from urllib.parse import urljoin

def get_enforcement_actions(start_month, start_year):
    # Check if the year is 2013 or later
    if start_year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions from 2013 onward are available.")
        return None
    
    # Generate start date from month and year
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime.today()
    
    # Log or return the start date and year
    print(f"Starting scraping from {start_month}/{start_year} to today: {end_date.strftime('%m/%d/%Y')}")
    
    # Placeholder DataFrame to store results (assuming actual scraping logic fills this)
    all_data = pd.DataFrame()
    
    
    url = 'https://oig.hhs.gov/fraud/enforcement/'
    response = requests.get(url)
    
    if response.status_code != 200:
        print("Failed to retrieve the page.")
        return None
    
    # Convert to BeautifulSoup
    soup = BeautifulSoup(response.text, 'lxml')

    # Scrape titles
    titles = soup.find_all('h2', class_='usa-card__heading')
    titles_text = [item.get_text(strip=True) for item in titles]

    # Scrape dates
    dates = soup.find_all('span', class_='text-base-dark padding-right-105')
    dates_text = [item.get_text(strip=True) for item in dates]

    # Scrape categories
    categories = soup.find_all('ul', class_="display-inline add-list-reset")
    categories_text = [item.get_text(strip=True) for item in categories]

    # Scrape links
    links = soup.find_all('a', href=True)
    links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
    filtered_links = links[3:23]  # Adjust as needed based on how many links you need
    links_url = [item.get('href') for item in filtered_links]

    # Add base URL to links
    base_url = 'https://oig.hhs.gov/'
    links_url = [urljoin(base_url, item) for item in links_url]

    # Assemble scraped elements into DataFrame
    data = {
        'Title': titles_text,
        'Date': dates_text,
        'Category': categories_text,
        'Link': links_url
    }
    
    # Convert to DataFrame
    data = pd.DataFrame(data)

    # scrape the agency names for each link
    agencies = []
    for link in links_url:
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Scrape agency name from the page
        agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")
        for span in agency_elements:
            agency_name = span.find_next_sibling(text=True).strip()
            agencies.append(agency_name)
        
        # Sleep to prevent being blocked (scraping multiple pages)
        time.sleep(1)
    
    # Update DataFrame with agency names
    # Ensure have the same number of agency names as the number of rows
    if len(agencies) == len(data):
        data['Agency'] = agencies
    else:
        print(f"Warning: The number of agencies ({len(agencies)}) does not match the number of enforcement actions ({len(data)})")

    # Filter data based on the start date
    # Assuming `data['Date']` needs to be converted to datetime objects first
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    filtered_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]
    
    # Sort by date
    filtered_data = filtered_data.sort_values(by='Date').reset_index(drop=True)

    # Save to CSV
    csv_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    filtered_data.to_csv(csv_filename, index=False)
    print(f"Data saved to {csv_filename}")

    return filtered_data
```


```{python}

```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```