---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
#setup
import pandas as pd
import altair as alt
import time
import requests
import lxml
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# make get request from website
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

# convert into soup
soup = BeautifulSoup(response.text, 'lxml')

# scrape title of enforcement actions
# find h2 tags with specific class 
titles = soup.find_all('h2', class_='usa-card__heading')
len(titles)
#scrape elements
titles_text = [item.get_text(strip=True) for item in titles]

#scrape elements of date
dates = soup.find_all('span', class_='text-base-dark padding-right-105')
dates_text = [item.get_text(strip=True) for item in dates]

#scrape elements of category
categories = soup.find_all('ul', class_="display-inline add-list-reset")
categories_text = [item.get_text(strip=True) for item in categories]

#scrape elements of links
links = soup.find_all('a', href = True)
links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
filtered_links=links[3:23]
links_url = [item.get('href') for item in links]

#add baseline url to links
base_url = 'https://oig.hhs.gov/'
links_url = [urljoin(base_url, item.get('href')) for item in filtered_links]

#assemble scraped elements into dataframe
data = {
    'Title': titles_text,
    'Date': dates_text,
    'Category': categories_text,
    'Link': links_url
}

data = pd.DataFrame(data)

print(data.head())
```

```{python}
agencies = []
for url in links_url:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")

    if agency_elements: 
      for span in agency_elements: 
        agency_name = span.find_next_sibling(text=True).strip()
        agencies.append(agency_name)
    else: 
      agencies.append('N/A')        

#update agencies on dataframe
data['agency_name'] = agencies        
```

  
### 2. Crawling (PARTNER 1)

```{python}

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)
(1)start
(2) check the start_year is before 2013, if it is then it prints a message and stops execution, because only data from 2013 onward is available.
(3) start_date is set to the specified date and end_date is set to today's date
(4) Then I create an empty DataFrame to store all scraped records. base_url: Defines the base URL for scraping. 'page'Sets the page number to start at 1.
(5) Use BeautifulSoup to parse the HTML content of the page with lxml parser
(6) Then scrape tites, dates, categories, links and agency names and then create a dataframe called 'data'
(7) Convert date column to datetime format then do filtering and sorting.
(8) Implement if statement to check the earliest date on the current page. If it is earlier than start_date, it breaks out of the loop.
(9) Increment the page number to scrape the next page. Use time.sleep for a 1 second to prevent overwhelming the server. 
(10) Filter the data to include only records within start_date to end_date
(11) Save filtered_data to a CSV file with this name enforcement_actions_startyear_startmonth.csv. 


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import time
import pandas as pd
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

def get_enforcement_actions(start_month, start_year):
    # Check if the year is 2013 or later
    if start_year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions from 2013 onward are available.")
        return None
    
    # Generate start date from month and year
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime.today()
    
    # Log the scraping start
    print(f"Starting scraping from {start_month}/{start_year} to today: {end_date.strftime('%m/%d/%Y')}")
    
    # Placeholder DataFrame to store results
    all_data = pd.DataFrame()
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    
    page = 1  # Start at the first page
    while True:
        # Construct the URL with the current page number
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        if response.status_code != 200:
            print("Failed to retrieve the page.")
            break
        
        # Convert to BeautifulSoup
        soup = BeautifulSoup(response.text, 'lxml')

        # Scrape titles
        titles = soup.find_all('h2', class_='usa-card__heading')
        titles_text = [item.get_text(strip=True) for item in titles]

        # Scrape dates
        dates = soup.find_all('span', class_='text-base-dark padding-right-105')
        dates_text = [item.get_text(strip=True) for item in dates]

        # Scrape categories
        categories = soup.find_all('ul', class_="display-inline add-list-reset")
        categories_text = [item.get_text(strip=True) for item in categories]

        # Scrape links
        links = soup.find_all('a', href=True)
        links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
        links_url = [urljoin(base_url, item.get('href')) for item in links[3:23]]

        # Assemble scraped elements into DataFrame
        data = pd.DataFrame({
            'Title': titles_text,
            'Date': dates_text,
            'Category': categories_text,
            'Link': links_url
        })

        # Convert 'Date' column to datetime, making sure it’s in a standard format
        data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%B %d, %Y')
        all_data = pd.concat([all_data, data], ignore_index=True)

        # Break if the earliest date on this page is older than `start_date`
        if data['Date'].min() < start_date:
            break

        # Increment to the next page
        page += 1

        # Sleep to prevent overwhelming the server
        time.sleep(1)

    # Filter by start date and sort
    filtered_data = all_data[(all_data['Date'] >= start_date) & (all_data['Date'] <= end_date)]
    filtered_data = filtered_data.sort_values(by='Date').reset_index(drop=True)

    # Save to CSV
    csv_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    filtered_data.to_csv(csv_filename, index=False)
    print(f"Data saved to {csv_filename}")

    return filtered_data

```

NEW CODE: 

```{python}
def get_enforcement_actions(start_month, start_year):
    # Check if the year is 2013 or later
    if start_year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions from 2013 onward are available.")
        return None
    
    # Generate start date from month and year
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime.today()
    
    # Log the scraping start
    print(f"Starting scraping from {start_month}/{start_year} to today: {end_date.strftime('%m/%d/%Y')}")
    
    # Placeholder DataFrame to store results
    all_data = pd.DataFrame()
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    
    page = 1  # Start at the first page
    while True:
        # Construct the URL with the current page number
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        if response.status_code != 200:
            print("Failed to retrieve the page.")
            break
        
        # Convert to BeautifulSoup
        soup = BeautifulSoup(response.text, 'lxml')

        # Scrape titles
        titles = soup.find_all('h2', class_='usa-card__heading')
        titles_text = [item.get_text(strip=True) for item in titles]

        # Scrape dates
        dates = soup.find_all('span', class_='text-base-dark padding-right-105')
        dates_text = [item.get_text(strip=True) for item in dates]

        # Scrape categories
        categories = soup.find_all('ul', class_="display-inline add-list-reset")
        categories_text = [item.get_text(strip=True) for item in categories]

        # Scrape links
        links = soup.find_all('a', href=True)
        links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
        links_url = [urljoin(base_url, item.get('href')) for item in links[3:23]]

        #scrape agency names
        agencies = []
        for url in links_url:
          response = requests.get(url)
          soup = BeautifulSoup(response.content, 'html.parser')
          agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")

        if agency_elements: 
          for span in agency_elements: 
            agency_name = span.find_next_sibling(text=True).strip()
            agencies.append(agency_name)
          else: 
            agencies.append('N/A')        


        # Assemble scraped elements into DataFrame
        data = pd.DataFrame({
            'Title': titles_text,
            'Date': dates_text,
            'Category': categories_text,
            'Link': links_url
        })

        # Convert 'Date' column to datetime, making sure it’s in a standard format
        data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%B %d, %Y')
        all_data = pd.concat([all_data, data], ignore_index=True)

        # Break if the earliest date on this page is older than `start_date`
        if data['Date'].min() < start_date:
            break

        # Increment to the next page
        page += 1

        # Sleep to prevent overwhelming the server
        time.sleep(1)

    # Filter by start date and sort
    filtered_data = all_data[(all_data['Date'] >= start_date) & (all_data['Date'] <= end_date)]
    filtered_data = filtered_data.sort_values(by='Date').reset_index(drop=True)

    # Save to CSV
    csv_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    filtered_data.to_csv(csv_filename, index=False)
    print(f"Data saved to {csv_filename}")

    return filtered_data
```

```{python}
get_enforcement_actions(1, 2023)
```

* c. Test Partner's Code (PARTNER 1)

```{python}
get_enforcement_actions(1, 2021)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}
```{python}
#read in data
enforcement_actions = pd.read_csv('enforcement_actions_2021_1.csv')

# Convert to datetime
enforcement_actions['Date'] = pd.to_datetime(enforcement_actions['Date'], errors='coerce')

#create month-year column
enforcement_actions['YearMonth'] = enforcement_actions['Date'].dt.to_period('M')
# Convert back to datetime
enforcement_actions['YearMonth'] = enforcement_actions['YearMonth'].dt.to_timestamp()

# Group by 'YearMonth' and count the occurrences 
enforcement_monthly = enforcement_actions.groupby('YearMonth').size().reset_index(name='Count')

# Set the renderer to 'default'
alt.renderers.enable('default')
#CGPT: I pasted this error I was getting that wasn't displaying my plot. ValueError: Saving charts in 'png' format requires the vl-convert-python package: see https://altair-viz.github.io/user_guide/saving_charts.html#png-svg-and-pdf-format
 
# Create the chart
enforcement_time = alt.Chart(enforcement_monthly).mark_line().encode(
    x='YearMonth:T', 
    y='Count:Q'
).properties(
    title='Enforcement Actions Over Time (2021-2024)'
)

enforcement_time
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```