---
title: "title"
author: "author"
date: "date"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
#setup
import pandas as pd
import altair as alt
import time
import requests
import lxml
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime

import warnings 
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

```{python}
# make get request from website
url = 'https://oig.hhs.gov/fraud/enforcement/'
response = requests.get(url)

# convert into soup
soup = BeautifulSoup(response.text, 'lxml')

# scrape title of enforcement actions
# find h2 tags with specific class 
titles = soup.find_all('h2', class_='usa-card__heading')
len(titles)
#scrape elements
titles_text = [item.get_text(strip=True) for item in titles]

#scrape elements of date
dates = soup.find_all('span', class_='text-base-dark padding-right-105')
dates_text = [item.get_text(strip=True) for item in dates]

#scrape elements of category
categories = soup.find_all('ul', class_="display-inline add-list-reset")
categories_text = [item.get_text(strip=True) for item in categories]

#scrape elements of links
links = soup.find_all('a', href = True)
links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
filtered_links=links[3:23]
links_url = [item.get('href') for item in links]

#add baseline url to links
base_url = 'https://oig.hhs.gov/'
links_url = [urljoin(base_url, item.get('href')) for item in filtered_links]

#assemble scraped elements into dataframe
data = {
    'Title': titles_text,
    'Date': dates_text,
    'Category': categories_text,
    'Link': links_url
}

data = pd.DataFrame(data)

print(data.head())
```

```{python}
agencies = []
for url in links_url:
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")

    if agency_elements: 
      for span in agency_elements: 
        agency_name = span.find_next_sibling(text=True).strip()
        agencies.append(agency_name)
    else: 
      agencies.append('N/A')        

#update agencies on dataframe
data['agency_name'] = agencies        
```

  
### 2. Crawling (PARTNER 1)

```{python}

```

## Step 2: Making the scraper dynamic

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2)


* b. Create Dynamic Scraper (PARTNER 2)

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
from datetime import datetime
from urllib.parse import urljoin

def get_enforcement_actions(start_month, start_year):
    # Check if the year is 2013 or later
    if start_year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions from 2013 onward are available.")
        return None
    
    # Generate start date from month and year
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime.today()
    
    # Log or return the start date and year
    print(f"Starting scraping from {start_month}/{start_year} to today: {end_date.strftime('%m/%d/%Y')}")
    
    # Placeholder DataFrame to store results (assuming actual scraping logic fills this)
    all_data = pd.DataFrame()
    
    
    url = 'https://oig.hhs.gov/fraud/enforcement/'
    response = requests.get(url)
    
    if response.status_code != 200:
        print("Failed to retrieve the page.")
        return None
    
    # Convert to BeautifulSoup
    soup = BeautifulSoup(response.text, 'lxml')

    # Scrape titles
    titles = soup.find_all('h2', class_='usa-card__heading')
    titles_text = [item.get_text(strip=True) for item in titles]

    # Scrape dates
    dates = soup.find_all('span', class_='text-base-dark padding-right-105')
    dates_text = [item.get_text(strip=True) for item in dates]

    # Scrape categories
    categories = soup.find_all('ul', class_="display-inline add-list-reset")
    categories_text = [item.get_text(strip=True) for item in categories]

    # Scrape links
    links = soup.find_all('a', href=True)
    links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
    filtered_links = links[3:23]  # Adjust as needed based on how many links you need
    links_url = [item.get('href') for item in filtered_links]

    # Add base URL to links
    base_url = 'https://oig.hhs.gov/'
    links_url = [urljoin(base_url, item) for item in links_url]

    # Assemble scraped elements into DataFrame
    data = {
        'Title': titles_text,
        'Date': dates_text,
        'Category': categories_text,
        'Link': links_url
    }
    
    # Convert to DataFrame
    data = pd.DataFrame(data)

    # scrape the agency names for each link
    agencies = []
    for link in links_url:
        response = requests.get(link)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Scrape agency name from the page
        agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")
        for span in agency_elements:
            agency_name = span.find_next_sibling(text=True).strip()
            agencies.append(agency_name)
        
        # Sleep to prevent being blocked (scraping multiple pages)
        time.sleep(1)
    
    # Update DataFrame with agency names
    # Ensure have the same number of agency names as the number of rows
    if len(agencies) == len(data):
        data['Agency'] = agencies
    else:
        print(f"Warning: The number of agencies ({len(agencies)}) does not match the number of enforcement actions ({len(data)})")

    # Filter data based on the start date
    # Assuming `data['Date']` needs to be converted to datetime objects first
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    filtered_data = data[(data['Date'] >= start_date) & (data['Date'] <= end_date)]
    
    # Sort by date
    filtered_data = filtered_data.sort_values(by='Date').reset_index(drop=True)

    # Save to CSV
    csv_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    filtered_data.to_csv(csv_filename, index=False)
    print(f"Data saved to {csv_filename}")

    return filtered_data
```



NEW CODE!
```{python}
def get_enforcement_actions(start_month, start_year):
    # Check if the year is 2013 or later
    if start_year < 2013:
        print("Please enter a year >= 2013, as only enforcement actions from 2013 onward are available.")
        return None
    
    # Generate start date from month and year
    start_date = datetime(start_year, start_month, 1)
    end_date = datetime.today()
    
    # Log the scraping start
    print(f"Starting scraping from {start_month}/{start_year} to today: {end_date.strftime('%m/%d/%Y')}")
    
    # Placeholder DataFrame to store results
    all_data = pd.DataFrame()
    base_url = 'https://oig.hhs.gov/fraud/enforcement/'
    
    page = 1  # Start at the first page
    while True:
        # Construct the URL with the current page number
        url = f"{base_url}?page={page}"
        response = requests.get(url)
        if response.status_code != 200:
            print("Failed to retrieve the page.")
            break
        
        # Convert to BeautifulSoup
        soup = BeautifulSoup(response.text, 'lxml')

        # Scrape titles
        titles = soup.find_all('h2', class_='usa-card__heading')
        titles_text = [item.get_text(strip=True) for item in titles]

        # Scrape dates
        dates = soup.find_all('span', class_='text-base-dark padding-right-105')
        dates_text = [item.get_text(strip=True) for item in dates]

        # Scrape categories
        categories = soup.find_all('ul', class_="display-inline add-list-reset")
        categories_text = [item.get_text(strip=True) for item in categories]

        # Scrape links
        links = soup.find_all('a', href=True)
        links = [item for item in links if '/fraud/enforcement/' in item.get('href')]
        links_url = [urljoin(base_url, item.get('href')) for item in links[3:23]]

        #scrape agency names
        agencies = []
        for url in links_url:
          response = requests.get(url)
          soup = BeautifulSoup(response.content, 'html.parser')
          agency_elements = soup.find_all('span', class_='padding-right-2 text-base', string="Agency:")

        if agency_elements: 
          for span in agency_elements: 
            agency_name = span.find_next_sibling(text=True).strip()
            agencies.append(agency_name)
          else: 
            agencies.append('N/A')        


        # Assemble scraped elements into DataFrame
        data = pd.DataFrame({
            'Title': titles_text,
            'Date': dates_text,
            'Category': categories_text,
            'Link': links_url
        })

        # Convert 'Date' column to datetime, making sure itâ€™s in a standard format
        data['Date'] = pd.to_datetime(data['Date'], errors='coerce', format='%B %d, %Y')
        all_data = pd.concat([all_data, data], ignore_index=True)

        # Break if the earliest date on this page is older than `start_date`
        if data['Date'].min() < start_date:
            break

        # Increment to the next page
        page += 1

        # Sleep to prevent overwhelming the server
        time.sleep(1)

    # Filter by start date and sort
    filtered_data = all_data[(all_data['Date'] >= start_date) & (all_data['Date'] <= end_date)]
    filtered_data = filtered_data.sort_values(by='Date').reset_index(drop=True)

    # Save to CSV
    csv_filename = f"enforcement_actions_{start_year}_{start_month}.csv"
    filtered_data.to_csv(csv_filename, index=False)
    print(f"Data saved to {csv_filename}")

    return filtered_data
```

* c. Test Partner's Code (PARTNER 1)

```{python}

```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)

```{python}

```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)

* based on "Criminal and Civil Actions" vs. "State Enforcement Agencies"

```{python}

```

* based on five topics

```{python}

```

## Step 4: Create maps of enforcement activity

### 1. Map by State (PARTNER 1)

```{python}

```


### 2. Map by District (PARTNER 2)

```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
```{python}

```

### 2. Conduct spatial join
```{python}

```

### 3. Map the action ratio in each district
```{python}

```